{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359775cb",
   "metadata": {},
   "source": [
    "# Foundation Models for Earth Observation\n",
    "\n",
    "This notebook demonstrates using embeddings from pre-trained foundation models for crop type classification in agricultural monitoring.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load pre-computed embeddings from the Alpha Earth Foundation model\n",
    "- Visualize high-dimensional embeddings using false-color RGB composites\n",
    "- Apply unsupervised clustering to discover crop patterns\n",
    "- Train supervised classifiers for crop type classification with minimal samples\n",
    "- Test model generalization across geographically distant regions\n",
    "- Understand few-shot learning and domain adaptation\n",
    "\n",
    "## Why Crop Classification Matters\n",
    "\n",
    "Accurate, timely crop mapping is critical for:\n",
    "- **Food Security**: Monitor crop production and predict yields globally\n",
    "- **Agricultural Policy**: Support subsidy programs and crop insurance\n",
    "- **Climate Adaptation**: Track changing agricultural practices\n",
    "- **Resource Management**: Optimize water use and fertilizer application\n",
    "- **Trade & Markets**: Forecast commodity supplies\n",
    "\n",
    "Traditional crop mapping requires:\n",
    "- Extensive field surveys (expensive, slow)\n",
    "- Expert image interpretation (not scalable)\n",
    "- Lots of labeled training data (hard to collect)\n",
    "\n",
    "Foundation models enable rapid, scalable crop mapping with minimal ground truth!\n",
    "\n",
    "## What are Foundation Model Embeddings?\n",
    "\n",
    "Foundation models are large neural networks pre-trained on massive amounts of satellite imagery (millions of images globally). They learn to extract meaningful features without task-specific labels.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Embeddings**: Compact numerical representations (64 dimensions) that capture patterns in imagery\n",
    "- **Transfer Learning**: Knowledge learned from global data transfers to your specific region\n",
    "- **Few-Shot Learning**: Achieve good results with just a handful of labeled samples per class\n",
    "\n",
    "**Alpha Earth Foundation Model:**\n",
    "We'll use [Google Alpha Earth Embeddings](https://arxiv.org/pdf/2507.22291). Note: Not all foundation models are created equal! Always understand:\n",
    "- What data was used for pre-training?\n",
    "- What geographic regions and time periods?\n",
    "- What biases might exist?\n",
    "- What are the model's limitations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980981a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and import helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fcca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install numpy pandas matplotlib rasterio seaborn xarray pyproj dask rioxarray pystac-client planetary-computer scikit-learn pyarrow tqdm scipy leafmap -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def in_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "if in_colab():\n",
    "    !git clone https://github.com/rohansaw/FoundationModels4EO.git\n",
    "    %cd FoundationModels4EO\n",
    "else:\n",
    "    print(\"Running locally - skipping git clone\")\n",
    "\n",
    "# Import our helper functions\n",
    "from geo_helpers import (\n",
    "    load_sentinel2_rgb,\n",
    "    load_sentinel2_rgb_timeseries,\n",
    "    load_crop_labels,\n",
    "    load_foundation_model_embeddings,\n",
    "    create_embedding_rgb,\n",
    "    simplify_crop_labels,\n",
    "    align_labels_to_embeddings,\n",
    "    prepare_training_data,\n",
    "    get_class_names,\n",
    "    print_crop_statistics,\n",
    "    prepare_csv_samples,\n",
    "    show_split_statistics,\n",
    "    compute_clusters,\n",
    "    predict_on_embeddings,\n",
    "    prepare_sample_coordinates\n",
    ")\n",
    "\n",
    "from viz_helpers import (\n",
    "    plot_rgb_image,\n",
    "    plot_rgb_timeseries,\n",
    "    plot_crop_labels,\n",
    "    plot_embeddings_rgb,\n",
    "    plot_clustering_results,\n",
    "    plot_classification_results,\n",
    "    plot_prediction_map,\n",
    "    plot_generalization_comparison,\n",
    "    show_study_area_map,\n",
    "    plot_classification_vs_clustering,\n",
    "    plot_sample_map_by_class,\n",
    "    plot_sample_map_by_split,\n",
    "    plot_classification_vs_clustering\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711ed0e",
   "metadata": {},
   "source": [
    "## 1. Define Study Area\n",
    "\n",
    "**Background:** The Mississippi Delta is one of the most productive agricultural regions in the United States, known for:\n",
    "- Extensive corn and soybean production\n",
    "- Flat terrain ideal for mechanized farming\n",
    "- Rich soils from the Mississippi River\n",
    "- Humid subtropical climate with long growing season\n",
    "\n",
    "**Your Task:**\n",
    "- Run the cell below to visualize our study area\n",
    "- Examine the interactive map\n",
    "- Note the agricultural landscape patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93383bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mississippi Delta region - corn/soy agricultural area\n",
    "LON_MIN, LON_MAX = -90.90, -90.75\n",
    "LAT_MIN, LAT_MAX = 33.45, 33.60\n",
    "YEAR = 2024\n",
    "\n",
    "# Visualize the study area on an interactive map\n",
    "print(\"Study Area\")\n",
    "show_study_area_map(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45512ec",
   "metadata": {},
   "source": [
    "## 2. Load Satellite Imagery and Embeddings\n",
    "\n",
    "We'll load two types of data:\n",
    "\n",
    "**1. Sentinel-2 L2A RGB Imagery:**\n",
    "- Free, open satellite imagery from the European Space Agency\n",
    "- 10-meter spatial resolution\n",
    "- Revisit every 5 days (with two satellites)\n",
    "- Loaded from [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) via STAC API\n",
    "\n",
    "**2. Google Alpha Earth Foundation Model Embeddings:**\n",
    "- Pre-computed 64-dimensional feature vectors for each pixel\n",
    "- Loaded from [Source Cooperative](https://source.coop/tge-labs/aef) (Courtesy of [Taylor Geospatial Engine Labs](https://tgengine.org/))\n",
    "\n",
    "**Your Task:**\n",
    "- Run the cells to load both datasets\n",
    "- Compare RGB imagery across the growing season\n",
    "- Note how embeddings compress complex patterns into 64 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec06f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentinel-2 RGB imagery for growing season\n",
    "# Mississippi growing season: April-October for corn/soy\n",
    "GROWING_SEASON_MONTHS = [4, 5, 6, 7, 8, 9, 10]  # Apr-Oct\n",
    "\n",
    "rgb_timeseries = load_sentinel2_rgb_timeseries(\n",
    "    LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, YEAR, GROWING_SEASON_MONTHS\n",
    ")\n",
    "\n",
    "plot_rgb_timeseries(rgb_timeseries, \"Mississippi Delta\", YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c17a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load foundation model embeddings (64-dimensional feature vector for each pixel)\n",
    "# This may take up to 3 minutes depending on your internet connection\n",
    "embeddings = load_foundation_model_embeddings(LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, YEAR)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"  - 64 feature dimensions capturing crop characteristics\")\n",
    "print(f\"  - {embeddings.shape[1]} x {embeddings.shape[2]} pixels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a968b",
   "metadata": {},
   "source": [
    "## 3. Visualize Embeddings\n",
    "\n",
    "**Challenge:** Embeddings have 64 dimensions, but we can only see in 3D (RGB)!\n",
    "\n",
    "**Solution:** Create a false-color RGB composite by selecting 3 embedding dimensions and mapping them to red, green, and blue channels.\n",
    "\n",
    "**What to look for:**\n",
    "- Different crops may appear in different colors\n",
    "- Field boundaries and patterns\n",
    "- How different dimensions highlight different features\n",
    "\n",
    "**Your Task:**\n",
    "- Run the cell with default dimensions [0, 10, 2]\n",
    "- **Experiment:** Try different dimension combinations\n",
    "  - Example: [5, 15, 25], [1, 30, 50], [10, 20, 40]. What do you note?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9714df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RGB visualization from 3 embedding dimensions\n",
    "dimensions_to_visualize = [0, 10, 2]  # Try changing these!\n",
    "\n",
    "embedding_rgb = create_embedding_rgb(embeddings, bands=dimensions_to_visualize)\n",
    "plot_embeddings_rgb(embedding_rgb, bands=dimensions_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d8a54",
   "metadata": {},
   "source": [
    "**Reflection Questions:**\n",
    "1. How does the embedding visualization compare to the RGB satellite imagery?\n",
    "2. Can you identify different crop types based on color patterns?\n",
    "3. Are field boundaries clearly visible?\n",
    "4. What do you think each dimension might be capturing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7647c1",
   "metadata": {},
   "source": [
    "## 4. Unsupervised Clustering\n",
    "\n",
    "**What is Clustering?**\n",
    "Clustering groups similar pixels together without using labels. It's completely unsupervised - the algorithm discovers natural patterns in the data.\n",
    "\n",
    "**Why use clustering?**\n",
    "- Explore data before labeling\n",
    "- Discover unexpected patterns\n",
    "- Validate that different crops have distinct signatures\n",
    "- Create initial labels for semi-supervised learning\n",
    "\n",
    "**What patterns might emerge?**\n",
    "- Fields with the same crop type\n",
    "- Built-up areas vs vegetation\n",
    "- Water bodies\n",
    "- Different crop growth stages\n",
    "- Soil types\n",
    "\n",
    "**Your Task:**\n",
    "- Run clustering with k=[3, 5, 10]\n",
    "- **Experiment:** Try different numbers of clusters (e.g., [3, 7, 15])\n",
    "- More clusters = finer distinctions but longer processing time\n",
    "- Are you able to identify what different clusters represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try k-means clustering with different numbers of clusters\n",
    "number_of_clusters_to_explore = [3, 5, 10]  # Try changing this!\n",
    "\n",
    "# Compute clusters\n",
    "cluster_results = compute_clusters(embeddings, k_values=number_of_clusters_to_explore)\n",
    "\n",
    "# Plot results\n",
    "plot_clustering_results(cluster_results=cluster_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6597e",
   "metadata": {},
   "source": [
    "**Analysis Questions:**\n",
    "1. What happens when you increase the number of clusters?\n",
    "2. With k=3, can you identify broad categories (vegetation vs non-vegetation)?\n",
    "3. With k=10, do you see finer distinctions between crop types?\n",
    "4. Do cluster boundaries align with field boundaries?\n",
    "5. What are the limitations of unsupervised clustering for crop classification?\n",
    "6. Would you need labeled data to use these clusters for crop mapping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b05c45",
   "metadata": {},
   "source": [
    "## 5. Supervised Learning: Train a Crop Classifier\n",
    "\n",
    "**From Unsupervised to Supervised:**\n",
    "Clustering revealed patterns, but we need labels to identify specific crops. Now we'll train a supervised classifier using labeled samples.\n",
    "\n",
    "**About the Training Data:**\n",
    "- Pre-extracted AlphaEarth embeddings for point locations across Mississippi\n",
    "- Labels from USDA Cropland Data Layer (CDL)\n",
    "- **Important:** CDL is model-predicted, not field-validated ground truth\n",
    "- 1,000 samples per class available\n",
    "- We'll use only 20 training samples per class to demonstrate few-shot learning!\n",
    "\n",
    "**Three Classes:**\n",
    "- **Corn**: Major grain crop\n",
    "- **Soy (Soybeans)**: Major oilseed and protein crop\n",
    "- **Other**: Everything else (other crops, forest, urban, water, etc.)\n",
    "\n",
    "**Your Task:**\n",
    "- Load the pre-labeled CSV data\n",
    "- Examine the class distribution\n",
    "- Prepare balanced train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-extracted Mississippi embeddings\n",
    "\n",
    "samples_mississipi = pd.read_csv(\"demo_data/mississippi_alphaearth_2024.csv\")\n",
    "print(f\"Loaded {len(samples_mississipi)} samples with embeddings\")\n",
    "print(f\"\\nClasses: {samples_mississipi['class_name'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and test splits\n",
    "# TODO: Experiment with different numbers of training samples!\n",
    "NUM_SAMPLES = {\n",
    "    'corn': {'label': 0, 'n_train': 20, 'n_test': 100},\n",
    "    'soy': {'label': 1, 'n_train': 20, 'n_test': 100},\n",
    "    'other': {'label': 2, 'n_train': 20, 'n_test': 50}\n",
    "}\n",
    "\n",
    "X_train, y_train, X_test, y_test, train_idx, test_idx = \\\n",
    "    prepare_csv_samples(samples_mississipi, NUM_SAMPLES, random_seed=42)\n",
    "\n",
    "show_split_statistics(samples_mississipi, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83edd220",
   "metadata": {},
   "source": [
    "### Spatial Distribution of Samples\n",
    "\n",
    "Understanding spatial distribution matters:\n",
    "- Identifying spatial autocorrelation (nearby samples are often similar)\n",
    "- Detecting spatial bias\n",
    "- Understanding coverage of training vs test data\n",
    "\n",
    "We'll create interactive maps showing samples and their train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816aae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sample coordinates for visualization\n",
    "samples_mississipi = prepare_sample_coordinates(samples_mississipi, train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30449ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all samples by crop class\n",
    "m_all = plot_sample_map_by_class(samples_mississipi, center_lat=33.5, center_lon=-90.8, zoom=7)\n",
    "m_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train vs test split\n",
    "m_split = plot_sample_map_by_split(samples_mississipi, center_lat=33.5, center_lon=-90.8, zoom=7)\n",
    "m_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb10921",
   "metadata": {},
   "source": [
    "### Understanding Spatial Autocorrelation\n",
    "\n",
    "**Tobler's First Law of Geography:**\n",
    "> \"Everything is related to everything else, but near things are more related than distant things\"\n",
    "\n",
    "**Spatial autocorrelation** means observations at nearby locations tend to be similar.\n",
    "\n",
    "**Why this matters for machine learning:**\n",
    "1. **Inflated accuracy metrics**: Train/test samples close together make models look better than they are\n",
    "2. **Location-specific learning**: Models may learn local patterns that don't generalize\n",
    "3. **Violates i.i.d. assumption**: Standard ML assumes independent, identically distributed samples\n",
    "4. **Overestimates generalization**: Model may fail in new regions despite high validation accuracy\n",
    "\n",
    "**In agricultural classification:**\n",
    "- Nearby fields often have the same crop (farm management patterns)\n",
    "- Environmental conditions (soil, climate) vary smoothly across space\n",
    "- Regional agricultural practices and crop rotations\n",
    "- Satellite imagery has spatial correlation in adjacent pixels\n",
    "\n",
    "Keep this in mind when working with ML solutions in the Geo Field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Training Random Forest classifier...\")\n",
    "print(f\"Using only {len(X_train)} training samples (20 per class)!\")\n",
    "print(\"This demonstrates the power of foundation model embeddings.\\n\")\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Show results\n",
    "plot_classification_results(y_test, y_pred, accuracy, ['other', 'corn', 'soy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0878c01f",
   "metadata": {},
   "source": [
    "**Analysis Questions:**\n",
    "1. How well does the model perform with only 20 training samples per class?\n",
    "2. Which classes are most confused with each other? (Look at the confusion matrix)\n",
    "3. Why might \"other\" be harder to classify than corn or soy?\n",
    "4. How many samples would traditional ML (without foundation models) need?\n",
    "5. What does this tell you about the quality of the embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96de477",
   "metadata": {},
   "source": [
    "## 6. Apply Classifier to Our Study Area\n",
    "\n",
    "**From Points to Maps:**\n",
    "So far we've classified individual sample points. Now let's create a wall-to-wall crop map for the entire study area!\n",
    "\n",
    "**What we'll do:**\n",
    "1. Apply our trained classifier to every pixel's embedding\n",
    "2. Generate a classified crop map\n",
    "3. Compare with unsupervised clustering results\n",
    "\n",
    "**Your Task:**\n",
    "- Run the prediction\n",
    "- Examine the output statistics (% of each crop)\n",
    "- Compare the supervised map with clustering from earlier\n",
    "- Look for patterns: field boundaries, roads, crop distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82291c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply classifier to spatial embeddings from our study area\n",
    "\n",
    "print(\"Predicting crop types for study area...\")\n",
    "crop_predictions = predict_on_embeddings(clf, embeddings)\n",
    "\n",
    "print(f\" Predictions complete!\")\n",
    "print(f\"Map shape: {crop_predictions.shape}\")\n",
    "print(f\"\\nPredicted crop distribution:\")\n",
    "for label, name in enumerate(['other', 'corn', 'soy']):\n",
    "    count = np.sum(crop_predictions == label)\n",
    "    pct = 100 * count / crop_predictions.size\n",
    "    print(f\"  {name.capitalize()}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare classification with clustering using precomputed clusters\n",
    "\n",
    "print(\"Comparing supervised classification with unsupervised clustering...\")\n",
    "\n",
    "# Display side-by-side comparison\n",
    "plot_classification_vs_clustering(\n",
    "    embeddings, \n",
    "    None,\n",
    "    crop_predictions, \n",
    "    {5: cluster_results[5]},  # Only use k=5 cluster from precomputed results\n",
    "    class_names=['other', 'corn', 'soy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f639f",
   "metadata": {},
   "source": [
    "**Understanding Limitations**\n",
    "\n",
    "**Observation:** The unsupervised clustering looks cleaner - field borders and roads are clearly visible. Why doesn't the supervised model show the same clarity?\n",
    "\n",
    "**Root Cause - Training Data Quality:**\n",
    "This highlights a critical lesson: **understand your training data, not just validation metrics!**\n",
    "\n",
    "Our labeled samples come from the **USDA Cropland Data Layer (CDL)**:\n",
    "- 30m spatial resolution (coarser than our 10m embeddings)\n",
    "- Roads and narrow features aren't always clearly visible\n",
    "- Road pixels often merged with adjacent field pixels\n",
    "- The model never learned to separate roads from crops\n",
    "\n",
    "**Result:** The classifier likely labels road pixels as \"corn\" / \"soy\" / rest based on nearby fields.\n",
    "\n",
    "**Key Takeaway:** \n",
    "- High accuracy on test data doesn't mean perfect predictions everywhere\n",
    "- Training data quality and resolution matter enormously\n",
    "- Always validate predictions against ground truth or high-resolution imagery\n",
    "- Understand the provenance and limitations of your labels\n",
    "\n",
    "**In practice, you should:**\n",
    "- Use higher-resolution ground truth when possible\n",
    "- Validate predictions with field data\n",
    "- Be aware of label noise and its propagation\n",
    "- Consider active learning to improve labels iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3add55",
   "metadata": {},
   "source": [
    "## 7. Testing Model Generalization Across Regions\n",
    "\n",
    "### Can Foundation Models Generalize?\n",
    "\n",
    "We trained our classifier with samples from Mississippi and achieved great results! But in real-world applications (especially disaster response), we often need models to work in completely new regions.\n",
    "\n",
    "**The Challenge:** Mississippi and Minnesota are ~1,400km apart with very different environmental conditions:\n",
    "\n",
    "| Factor | Mississippi | Minnesota |\n",
    "|--------|-------------|-----------|\n",
    "| **Latitude** | 30.2N - 35.0°N | 43.5°N - 49.4°N |\n",
    "| **Climate** | Humid Subtropical | Continental |\n",
    "| **Growing Season** | 240-270 days | 120-160 days |\n",
    "| **Summer Temperature** | 27-29°C | 20-23°C |\n",
    "| **Winter Temperature** | 8-11°C | -12 to -7°C |\n",
    "| **Annual Rainfall** | 1,400-1,600 mm | 500-750 mm |\n",
    "| **Soil Type** | Clay loam, alluvial | Mollisols, glacial till |\n",
    "\n",
    "**Why this matters for ML:**\n",
    "- Different spectral signatures due to climate\n",
    "- Different crop phenology (growth timing)\n",
    "- Different stress patterns (heat vs cold)\n",
    "- Testing true generalization capability\n",
    "\n",
    "These differences create a \"domain shift\" - can our model handle it? Let's find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c3ac6",
   "metadata": {},
   "source": [
    "### Load Minnesota Data for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c930fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Minnesota data\n",
    "samples_minnesota = pd.read_csv(\"demo_data/minnesota_alphaearth_2024.csv\")\n",
    "\n",
    "# Prepare test set (300 samples per class)\n",
    "embedding_columns = [col for col in samples_minnesota.columns if col.startswith('A')]\n",
    "X_minnesota_all = samples_minnesota[embedding_columns].values\n",
    "y_minnesota_all = samples_minnesota['label'].values\n",
    "\n",
    "# Sample test set\n",
    "np.random.seed(42)\n",
    "test_indices_mn = []\n",
    "for label in [0, 1, 2]:\n",
    "    class_idx = np.where(y_minnesota_all == label)[0]\n",
    "    sampled = np.random.choice(class_idx, min(300, len(class_idx)), replace=False)\n",
    "    test_indices_mn.extend(sampled)\n",
    "\n",
    "test_indices_mn = np.array(test_indices_mn)\n",
    "np.random.shuffle(test_indices_mn)\n",
    "\n",
    "X_test_minnesota = X_minnesota_all[test_indices_mn]\n",
    "y_test_minnesota = y_minnesota_all[test_indices_mn]\n",
    "\n",
    "print(f\"Minnesota test set: {len(X_test_minnesota)} samples\")\n",
    "for label in [0, 1, 2]:\n",
    "    count = np.sum(y_test_minnesota == label)\n",
    "    name = samples_minnesota[samples_minnesota['label'] == label]['class_name'].iloc[0]\n",
    "    print(f\"  {name}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d298b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Mississippi-trained model on Minnesota (Zero-Shot Transfer)\n",
    "print(\"Testing zero-shot transfer: Mississippi model -> Minnesota data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "y_pred_zeroshot = clf.predict(X_test_minnesota)\n",
    "accuracy_zeroshot = accuracy_score(y_test_minnesota, y_pred_zeroshot)\n",
    "\n",
    "print(f\"\\n Zero-Shot Accuracy: {accuracy_zeroshot:.2%}\")\n",
    "print(f\"\\n  Trained on: {len(X_train)} Mississippi samples\")\n",
    "print(f\"  Tested on: {len(X_test_minnesota)} Minnesota samples\")\n",
    "print(f\"  Distance: ~1,400 km apart\")\n",
    "\n",
    "# Show results\n",
    "plot_classification_results(y_test_minnesota, y_pred_zeroshot, accuracy_zeroshot, \n",
    "                           ['other', 'corn', 'soy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b91f0",
   "metadata": {},
   "source": [
    "### Few-Shot Adaptation\n",
    "\n",
    "**The Challenge:**\n",
    "Performance dropped significantly due to **distribution shift** (domain shift) between Mississippi and Minnesota:\n",
    "- Different climate and growing conditions\n",
    "- Different crop phenology (growth timing and patterns)\n",
    "- Different spectral signatures\n",
    "- Different agricultural practices\n",
    "\n",
    "**The Question:**\n",
    "Can we improve performance by adding just a few samples from the target region?\n",
    "\n",
    "**Few-Shot Learning:**\n",
    "Let's add only **5 Minnesota samples per class** (15 total) to our training set and see what happens!\n",
    "\n",
    "This simulates a real-world scenario:\n",
    "- Limited time/budget for field surveys in new region\n",
    "- Need rapid deployment (disaster response, early warning)\n",
    "- Foundation models should make this efficient\n",
    "\n",
    "**Your Task:**\n",
    "- Run the few-shot adaptation\n",
    "- Compare zero-shot vs few-shot accuracy\n",
    "- Analyze the improvement\n",
    "- Consider: Is 5 samples per class reasonable to collect in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329fab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 5 Minnesota samples per class to training set\n",
    "N_FEWSHOT = 5\n",
    "\n",
    "# Get samples not in test set\n",
    "remaining_idx = np.setdiff1d(np.arange(len(y_minnesota_all)), test_indices_mn)\n",
    "\n",
    "fewshot_indices = []\n",
    "for label in [0, 1, 2]:\n",
    "    class_remaining = remaining_idx[y_minnesota_all[remaining_idx] == label]\n",
    "    sampled = np.random.choice(class_remaining, N_FEWSHOT, replace=False)\n",
    "    fewshot_indices.extend(sampled)\n",
    "\n",
    "fewshot_indices = np.array(fewshot_indices)\n",
    "\n",
    "X_fewshot = X_minnesota_all[fewshot_indices]\n",
    "y_fewshot = y_minnesota_all[fewshot_indices]\n",
    "\n",
    "# Combine with Mississippi training data\n",
    "X_train_combined = np.vstack([X_train, X_fewshot])\n",
    "y_train_combined = np.concatenate([y_train, y_fewshot])\n",
    "\n",
    "print(f\"Combined training set:\")\n",
    "print(f\"  Mississippi: {len(X_train)} samples\")\n",
    "print(f\"  Minnesota: {len(X_fewshot)} samples\")\n",
    "print(f\"  Total: {len(X_train_combined)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035f9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train combined model\n",
    "print(\"Training model with Mississippi + Minnesota few-shot samples...\")\n",
    "\n",
    "clf_combined = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf_combined.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "# Test on Minnesota\n",
    "y_pred_fewshot = clf_combined.predict(X_test_minnesota)\n",
    "accuracy_fewshot = accuracy_score(y_test_minnesota, y_pred_fewshot)\n",
    "\n",
    "print(f\"\\n Few-Shot Accuracy: {accuracy_fewshot:.2%}\")\n",
    "print(f\"  Improvement: {accuracy_fewshot - accuracy_zeroshot:+.2%}\")\n",
    "\n",
    "# Show results\n",
    "plot_classification_results(y_test_minnesota, y_pred_fewshot, accuracy_fewshot,\n",
    "                           ['other', 'corn', 'soy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d4e0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "class_names = ['other', 'corn', 'soy']\n",
    "\n",
    "# Zero-shot confusion matrix\n",
    "cm_zero = confusion_matrix(y_test_minnesota, y_pred_zeroshot)\n",
    "sns.heatmap(cm_zero, annot=True, fmt='d', cmap='Reds', ax=axes[0],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[0].set_title(f'Zero-Shot Transfer\\n(Mississippi only)\\nAccuracy: {accuracy_zeroshot:.2%}',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Few-shot confusion matrix\n",
    "cm_few = confusion_matrix(y_test_minnesota, y_pred_fewshot)\n",
    "sns.heatmap(cm_few, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "axes[1].set_title(f'Few-Shot Adapted\\n(+5 MN samples/class)\\nAccuracy: {accuracy_fewshot:.2%}',\n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57922634",
   "metadata": {},
   "source": [
    "**Analysis Questions:**\n",
    "\n",
    "1. **Zero-Shot Performance:**\n",
    "   - How well did the Mississippi-trained model work on Minnesota?\n",
    "   - Which classes had the most errors?\n",
    "   - Why didn't it completely fail despite the domain shift?\n",
    "\n",
    "2. **Few-Shot Improvement:**\n",
    "   - How much did accuracy improve with just 5 samples per class?\n",
    "   - Is this improvement practically significant?\n",
    "   - Which classes benefited most from the few-shot samples?\n",
    "\n",
    "3. **Practical Implications:**\n",
    "   - Could you collect 15 samples (5 per class) in a day of field work?\n",
    "   - How does this compare to traditional approaches needing hundreds of samples?\n",
    "   - When would few-shot learning be especially valuable?\n",
    "\n",
    "4. **Foundation Model Benefits:**\n",
    "   - Why can foundation models learn from so few samples?\n",
    "   - What knowledge transferred from Mississippi to Minnesota?\n",
    "   - What had to be adapted with the few-shot samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a51fc1",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What We Learned About Foundation Models\n",
    "\n",
    "1. **Rich Embeddings Capture Crop Identity**\n",
    "   - 64-dimensional embeddings encode meaningful crop characteristics\n",
    "   - Pre-training on massive global datasets creates generalizable features\n",
    "   - Unsupervised clustering reveals natural patterns aligned with crop types\n",
    "   - Can be visualized as false-color images to interpret what the model \"sees\"\n",
    "\n",
    "2. **Efficient Training with Minimal Samples**\n",
    "   - Achieved high accuracy with only **20 training samples per class**\n",
    "   - Foundation models transfer knowledge from pre-training on millions of images\n",
    "   - Dramatically reduces field survey costs and time\n",
    "\n",
    "3. **Zero-Shot Transfer Works Across Regions**\n",
    "   - Model trained on Mississippi performed reasonably on Minnesota\n",
    "   - **~1,400km distance** with very different environmental conditions\n",
    "   - Demonstrates that foundation models learn generalizable crop features\n",
    "   - Not perfect, but provides a strong starting point for new regions\n",
    "\n",
    "4. **Few-Shot Adaptation is Powerful**\n",
    "   - Adding just **5 samples per class (15 total)** significantly improved performance\n",
    "   - Orders of magnitude less data than traditional approaches\n",
    "   - Critical for rapid deployment in new regions\n",
    "   - Enables agile response to emerging food security challenges\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "**Disaster Response & Early Warning:**\n",
    "- Quickly map crops in disaster-affected regions\n",
    "- Minimal field surveys needed\n",
    "- Rapid deployment for damage assessment\n",
    "\n",
    "**Global Agricultural Monitoring:**\n",
    "- Train models in data-rich areas\n",
    "- Deploy to data-poor regions worldwide\n",
    "- Monitor food production globally\n",
    "\n",
    "**Cost Efficiency:**\n",
    "- Reduce field campaign costs by 90%+\n",
    "- Smaller teams can cover larger areas\n",
    "- More frequent monitoring becomes feasible\n",
    "\n",
    "**Rapid Prototyping:**\n",
    "- Test classification approaches quickly\n",
    "- Validate feasibility before full data collection\n",
    "- Iterate on class definitions efficiently\n",
    "\n",
    "**Validation Strategy:**\n",
    "- This demo used simplified train/test splits\n",
    "- Real applications should use train/validation/test or k-fold cross-validation\n",
    "- Validate hyperparameters on separate data\n",
    "\n",
    "**Ground Truth Quality:**\n",
    "- CDL is predicted data, not perfect ground truth\n",
    "- Label quality directly impacts model performance\n",
    "- Always validate with field data when possible\n",
    "- Consider label noise in your accuracy expectations\n",
    "\n",
    "**Foundation Model Limitations:**\n",
    "- Pre-training data may not cover all regions equally\n",
    "- Temporal coverage varies\n",
    "- Biases exist based on training data and generalization can be limited!\n",
    "- Always validate in your specific context\n",
    "\n",
    "### Reflections & Discussion\n",
    "\n",
    "**Think about:**\n",
    "\n",
    "1. **Data Efficiency:**\n",
    "   - How would your workflow change with foundation models vs traditional ML?\n",
    "   - What field surveys become feasible that weren't before?\n",
    "   - How does this impact global food security monitoring?\n",
    "\n",
    "2. **Generalization:**\n",
    "   - When would zero-shot transfer work well vs poorly?\n",
    "   - What factors determine if few-shot learning is sufficient?\n",
    "   - How would you decide how many samples to collect?\n",
    "\n",
    "3. **Model Selection:**\n",
    "   - When would you use clustering vs supervised learning?\n",
    "   - Could you combine both approaches?\n",
    "   - What other classifiers might work better than Random Forest?\n",
    "\n",
    "4. **Operational Deployment:**\n",
    "   - How would you validate predictions before making decisions?\n",
    "   - What quality control steps would you implement?\n",
    "   - How would you handle prediction uncertainty?\n",
    "\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "**Foundation Models:**\n",
    "- [Alpha Earth Paper](https://arxiv.org/pdf/2507.22291): Technical details and benchmarks\n",
    "- [Prithvi](https://huggingface.co/ibm-nasa-geospatial): NASA-IBM geospatial foundation model\n",
    "- [Segment Anything Model](https://segment-anything.com/): For image segmentation tasks\n",
    "\n",
    "**Data Sources:**\n",
    "- [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/): Free satellite data access\n",
    "- [Google Earth Engine](https://earthengine.google.com/): Petabyte-scale analysis platform\n",
    "- [Source Cooperative](https://source.coop/): Open geospatial data repository\n",
    "\n",
    "**Agricultural Monitoring:**\n",
    "- [USDA NASS](https://www.nass.usda.gov/): Official US crop statistics\n",
    "- [GEOGLAM](https://www.geoglam.org/): Global agricultural monitoring initiative\n",
    "- [Crop Monitor](https://cropmonitor.org/): Monthly crop conditions worldwide\n",
    "\n",
    "**Machine Learning:**\n",
    "- [Scikit-learn](https://scikit-learn.org/): ML library documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beea6e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (other-project)",
   "language": "python",
   "name": "other-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
